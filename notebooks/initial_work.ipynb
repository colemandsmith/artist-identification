{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first run through this experimentation will be following the model from the paper [Artist Identification with Convulutional Neural Networks](http://cs231n.stanford.edu/reports/2017/pdfs/406.pdf), and working with the [same dataset from Kaggle](https://www.kaggle.com/c/painter-by-numbers). I will initially only be working with a subset of the dataset, train_1.zip, partly due to my ISP's data caps and my current computer setup. In the following month I hope to be able to download more of the data set. I will be using PyTorch for my neural network(s).\n",
    "\n",
    "UPDATE 06/30/18: I have decided to download the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In process.py we processed out all of the artists with less than 300 paintings in the dataset so as to ensure that there are sufficient samples for each artist to train on and learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from os.path import expanduser\n",
    "\n",
    "#trim the irrelevant files; make a csv of our subset of the data\n",
    "data_dir = expanduser(\"~\") +\"/Data/\"\n",
    "all_artist_data = data_dir + \"all_artist_data.csv\"\n",
    "filtered = data_dir + \"filtered.csv\"\n",
    "artist_train = data_dir + \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_artist = {}\n",
    "artist_to_label = {}\n",
    "label_counter = 0\n",
    "\n",
    "with open(filtered, \"r\", encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile, quotechar='\\\"')\n",
    "    header = next(reader)\n",
    "    for row in reader:\n",
    "        if row[0] not in artist_to_label:\n",
    "            label_to_artist[label_counter] = row[0]\n",
    "            artist_to_label[row[0]] = label_counter\n",
    "            label_counter += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaslineCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=6272, out_features=228, bias=True)\n",
      "  (fc2): Linear(in_features=228, out_features=38, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaslineCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BaslineCNN, self).__init__()\n",
    "        # 3 input channels, 32 output channels, 3x3 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(3,64,3, stride=2, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(64,64,3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(6272,228)\n",
    "        self.fc2 = nn.Linear(228,len(artist_to_label))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 6272)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "net = BaslineCNN()\n",
    "    \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to load in an transform our data into proper format. This involves implementing the Dataset asbtract class as well as the DataLoader class with versions specific to our data and our desired transformations. To follow along with the paper, we are going to randomly crop 224x244 images out of the training images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# we trust this dataset (I think?)\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "train_loader_transform = transforms.Compose([transforms.RandomCrop(224),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "\n",
    "class ArtistImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,text_file,img_dir,transform=train_loader_transform):\n",
    "        self.name_frame = pd.read_csv(text_file,sep=\",\",usecols=range(11,12))\n",
    "        self.label_frame = pd.read_csv(text_file,sep=\",\",usecols=range(1))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.name_frame)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = os.path.join(self.img_dir, self.name_frame.iloc[index, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        labels = artist_to_label[self.label_frame.iloc[index, 0]]\n",
    "        sample = {'images': image, 'labels': labels}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "artist_image_dataset = ArtistImageDataset(text_file=filtered, img_dir = artist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#split into train, test, and validation sets\n",
    "num_imgs = len(artist_image_dataset)\n",
    "indices = list(range(num_imgs))\n",
    "test_indices = np.random.choice(indices, size=num_imgs//10, replace=False)\n",
    "train_indices = list(set(indices) - set(test_indices))\n",
    "indices = train_indices\n",
    "validation_indices = np.random.choice(indices, size=num_imgs//10, replace=False)\n",
    "train_indices = list(set(indices) - set(validation_indices))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(artist_image_dataset,\n",
    "                                           batch_size=16,\n",
    "                                           num_workers=2,\n",
    "                                           sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(artist_image_dataset,\n",
    "                                           batch_size=16,\n",
    "                                           sampler=test_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(artist_image_dataset,\n",
    "                                         batch_size=16,\n",
    "                                         sampler=validation_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# get some random training images\\ndataiter = iter(train_loader)\\nsample = dataiter.next()\\n# show images\\nimshow(torchvision.utils.make_grid(sample['images']))\\n# print labels\\nprint([label_to_artist[s.data.numpy()[()]] for s in sample['labels']])\\nprint(sample['labels'])\\nprint(len(sample['labels']))\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# image-showing code taken from the PyTorch tutorial\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "'''\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "sample = dataiter.next()\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(sample['images']))\n",
    "# print labels\n",
    "print([label_to_artist[s.data.numpy()[()]] for s in sample['labels']])\n",
    "print(sample['labels'])\n",
    "print(len(sample['labels']))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#make sure we do things on the gpu\n",
    "net.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 3.506\n",
      "[1,   200] test accuracy: 9.571\n",
      "[1,   400] loss: 3.307\n",
      "[1,   400] test accuracy: 11.786\n",
      "[1,   600] loss: 3.207\n",
      "[1,   600] test accuracy: 14.071\n",
      "[2,   200] loss: 3.058\n",
      "[2,   200] test accuracy: 15.071\n",
      "[2,   400] loss: 3.069\n",
      "[2,   400] test accuracy: 17.429\n",
      "[2,   600] loss: 3.000\n",
      "[2,   600] test accuracy: 18.714\n",
      "[3,   200] loss: 2.942\n",
      "[3,   200] test accuracy: 17.571\n",
      "[3,   400] loss: 2.918\n",
      "[3,   400] test accuracy: 19.786\n",
      "[3,   600] loss: 2.877\n",
      "[3,   600] test accuracy: 20.714\n",
      "[4,   200] loss: 2.852\n",
      "[4,   200] test accuracy: 21.857\n",
      "[4,   400] loss: 2.798\n",
      "[4,   400] test accuracy: 21.714\n",
      "[4,   600] loss: 2.779\n",
      "[4,   600] test accuracy: 18.857\n",
      "[5,   200] loss: 2.798\n",
      "[5,   200] test accuracy: 21.929\n",
      "[5,   400] loss: 2.757\n",
      "[5,   400] test accuracy: 24.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-10:\n",
      "Process Process-9:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-5-d14fc63bffc4>\", line 26, in __getitem__\n",
      "    image = Image.open(img_name).convert('RGB')\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-5-d14fc63bffc4>\", line 26, in __getitem__\n",
      "    image = Image.open(img_name).convert('RGB')\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/cole/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bd9bb2b74b85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m199\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             print('[%d, %5d] loss: %.3f' %\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#re init\n",
    "train_loader = torch.utils.data.DataLoader(artist_image_dataset,\n",
    "                                           batch_size=16,\n",
    "                                           num_workers=2,\n",
    "                                           sampler=train_sampler)\n",
    "test_acc = []\n",
    "iterations = []\n",
    "train_len = len(train_loader)\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    for i, sample in enumerate(train_loader):\n",
    "        images, labels = sample['images'].cuda(), sample['labels'].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                for sample in test_loader:\n",
    "                    images,labels = sample['images'].cuda(), sample['labels'].cuda()\n",
    "                    outputs = net(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                test_acc.append(100*correct/total)\n",
    "                iterations.append((i+1) + (train_len * epoch))\n",
    "                print(\"[%d, %5d] test accuracy: %.3f\" % (epoch + 1, i + 1, 100*correct/total))\n",
    "                    \n",
    "print('done')\n",
    "plt.plot(iterations, test_acc)\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel('percent accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run this model against our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# no grad because we don't want our model to update during testing\n",
    "with torch.no_grad():\n",
    "    for sample in validation_loader:\n",
    "        images,labels = sample['images'].cuda(), sample['labels'].cuda()\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print('Accuracy of the network on the 1400 validation images: {0:.2f}'.format(100 * correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper noted getting better results using Adam vs SGD + Momentum, so I decided to check out the results on my own. I ended up finding SGD + Momentum to be better for this particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"first_real_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
