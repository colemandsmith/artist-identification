{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be sketching out my implementation of the Resnet-18 network used in the [paper](http://cs231n.stanford.edu/reports/2017/pdfs/406.pdf) mentioned in initial_work.ipynb. I want to try building it out with the base components built into PyTorch and compare that to PyTorch's built-in Resnet constructors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HomegrownResnet18(\n",
      "  (in_conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (bnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (chunk64): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (in_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (in_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (chunk128): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (in_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (in_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (chunk256): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (in_conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (in_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (chunk512): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (in_conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (in_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=2, padding=0)\n",
      "  (fc): Linear(in_features=512, out_features=38, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import common_utils\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Class representing one two-layer residual learning block, as outlined in the paper\n",
    "    \"Deep Residual Learning for Image Recognition\"\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dimension, dimension, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.in_dimension = in_dimension\n",
    "        self.in_conv = nn.Conv2d(in_dimension, dimension, 3, stride=stride, padding=1)\n",
    "        self.bnorm = nn.BatchNorm2d(dimension)\n",
    "        self.conv = nn.Conv2d(dimension, dimension, 3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Connectivity follows the diagram from the paper 'Deep Residual Learning for Image Recognition'\n",
    "        \"\"\"\n",
    "        res = x\n",
    "        out = F.relu(self.bnorm(self.in_conv(x)), inplace=True)\n",
    "        out = self.bnorm(self.conv(out))\n",
    "        # adding the residual weights to the layer output\n",
    "        # need to downsample the residual in first layer of blocks with nonzero stride\n",
    "        if (res.size() != out.size()):\n",
    "            conv = nn.Conv2d(self.in_dimension, self.dimension, 1, stride=2)\n",
    "            bnorm = nn.BatchNorm2d(self.dimension)\n",
    "            res = bnorm(conv(res))\n",
    "        out += res\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class HomegrownResnet18(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(HomegrownResnet18, self).__init__()\n",
    "        self.in_conv = nn.Conv2d(3, 64, 7, stride=2, padding=3)\n",
    "        self.bnorm = nn.BatchNorm2d(64)\n",
    "        self.max_pool = nn.MaxPool2d(3,stride=2, padding = 1)\n",
    "        self.chunk64 = nn.Sequential(*[ResBlock(64,64),ResBlock(64,64)])\n",
    "        self.chunk128 = nn.Sequential(*[ResBlock(64,128,stride=2),ResBlock(128,128)])\n",
    "        self.chunk256 = nn.Sequential(*[ResBlock(128,256,stride=2),ResBlock(256,256)])\n",
    "        self.chunk512 = nn.Sequential(*[ResBlock(256,512,stride=2),ResBlock(512,512)])\n",
    "        self.avgpool = nn.AvgPool2d(7,stride=2)\n",
    "        self.fc = nn.Linear(512,num_classes)\n",
    "        \n",
    "        # initialize weights in a way that makes sense for ReLU layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bnorm(self.in_conv(x)),inplace=True)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.chunk64(x)\n",
    "        x = self.chunk128(x)\n",
    "        x = self.chunk256(x)\n",
    "        x = self.chunk512(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "net = HomegrownResnet18(38)\n",
    "    \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = common_utils.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    80] loss: 3.498\n",
      "[1,    80] test accuracy: 6.786\n",
      "[1,   160] loss: 3.384\n",
      "[1,   160] test accuracy: 10.143\n",
      "[2,    80] loss: 3.337\n",
      "[2,    80] test accuracy: 12.643\n",
      "[2,   160] loss: 3.233\n",
      "[2,   160] test accuracy: 11.786\n",
      "[3,    80] loss: 3.240\n",
      "[3,    80] test accuracy: 12.643\n",
      "[3,   160] loss: 3.183\n",
      "[3,   160] test accuracy: 14.929\n",
      "[4,    80] loss: 3.169\n",
      "[4,    80] test accuracy: 15.571\n",
      "[4,   160] loss: 3.144\n",
      "[4,   160] test accuracy: 15.000\n",
      "[5,    80] loss: 3.138\n",
      "[5,    80] test accuracy: 17.143\n",
      "[5,   160] loss: 3.082\n",
      "[5,   160] test accuracy: 15.571\n",
      "[6,    80] loss: 3.068\n",
      "[6,    80] test accuracy: 16.429\n",
      "[6,   160] loss: 3.061\n",
      "[6,   160] test accuracy: 14.571\n",
      "[7,    80] loss: 3.015\n",
      "[7,    80] test accuracy: 18.357\n",
      "[7,   160] loss: 2.991\n",
      "[7,   160] test accuracy: 18.786\n",
      "[8,    80] loss: 2.984\n",
      "[8,    80] test accuracy: 20.286\n",
      "[8,   160] loss: 2.915\n",
      "[8,   160] test accuracy: 20.571\n",
      "[9,    80] loss: 2.892\n",
      "[9,    80] test accuracy: 22.000\n",
      "[9,   160] loss: 2.865\n",
      "[9,   160] test accuracy: 20.000\n",
      "[10,    80] loss: 2.872\n",
      "[10,    80] test accuracy: 20.429\n",
      "[10,   160] loss: 2.821\n",
      "[10,   160] test accuracy: 20.714\n",
      "[11,    80] loss: 2.790\n",
      "[11,    80] test accuracy: 22.571\n",
      "[11,   160] loss: 2.766\n",
      "[11,   160] test accuracy: 23.000\n",
      "[12,    80] loss: 2.744\n",
      "[12,    80] test accuracy: 24.000\n",
      "[12,   160] loss: 2.730\n",
      "[12,   160] test accuracy: 22.857\n",
      "[13,    80] loss: 2.701\n",
      "[13,    80] test accuracy: 24.214\n",
      "[13,   160] loss: 2.687\n",
      "[13,   160] test accuracy: 25.571\n",
      "[14,    80] loss: 2.664\n",
      "[14,    80] test accuracy: 25.214\n",
      "[14,   160] loss: 2.672\n",
      "[14,   160] test accuracy: 26.143\n",
      "[15,    80] loss: 2.673\n",
      "[15,    80] test accuracy: 25.000\n",
      "[15,   160] loss: 2.629\n",
      "[15,   160] test accuracy: 25.429\n",
      "[16,    80] loss: 2.601\n",
      "[16,    80] test accuracy: 28.429\n",
      "[16,   160] loss: 2.582\n",
      "[16,   160] test accuracy: 26.857\n",
      "[17,    80] loss: 2.617\n",
      "[17,    80] test accuracy: 27.429\n",
      "[17,   160] loss: 2.553\n",
      "[17,   160] test accuracy: 27.357\n",
      "[18,    80] loss: 2.573\n",
      "[18,    80] test accuracy: 28.071\n",
      "[18,   160] loss: 2.524\n",
      "[18,   160] test accuracy: 27.500\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "val_acc = []\n",
    "iterations = []\n",
    "train_len = len(train)\n",
    "\n",
    "for epoch in range(30):\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    for i, sample in enumerate(train):\n",
    "        images, labels = sample['images'], sample['labels']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 80 == 79:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 80))\n",
    "            running_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                for sample in val:\n",
    "                    images,labels = sample['images'], sample['labels']\n",
    "                    outputs = net(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                val_acc.append(100*correct/total)\n",
    "                iterations.append((i+1) + (train_len * epoch))\n",
    "                print(\"[%d, %5d] test accuracy: %.3f\" % (epoch + 1, i + 1, 100*correct/total))\n",
    "                    \n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iterations, val_acc)\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel('percent accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
